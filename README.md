**Overview**

The Multimodal Conversational Knowledge Agent is an intelligent assistant that processes and understands text, images, and speech inputs to deliver accurate, context-aware responses.
It combines OCR, speech-to-text, and Retrieval-Augmented Generation (RAG) techniques to enable natural, human-like academic or informational Q&A experiences.


**Key Features**

ğŸ—‚ï¸ Multimodal Input â€” Supports text, image (OCR), and speech input modes.

* ğŸ” Contextual Retrieval â€” Uses LangChain + FAISS for document search and semantic context matching.
* ğŸ§¾ OCR Processing â€” Extracts text from images or scanned notes using Tesseract OCR.
* ğŸ™ï¸ Speech-to-Text â€” Converts voice queries into text using SpeechRecognition API.
* ğŸ’¬ Conversational Memory â€” Retains session context for smooth multi-turn interactions.
* âš™ï¸ Streamlit UI â€” Simple, user-friendly web interface for uploading documents and chatting with the assistant.


**ğŸ’¡ Usage**

* Upload your document (PDF, image, or text).
* Optionally record or upload voice input for speech-based queries.
* Ask questions â€” the agent retrieves relevant content, interprets it, and generates concise, contextual answers.

**ğŸ§® Example Use Cases**

* Academic Q&A based on lecture notes or study material
* Conversational document retrieval
* AI-based note summarization
* Voice-assisted learning assistant
