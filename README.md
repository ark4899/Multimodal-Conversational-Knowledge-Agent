**Overview**

The Multimodal Conversational Knowledge Agent is an intelligent assistant that processes and understands text, images, and speech inputs to deliver accurate, context-aware responses.
It combines OCR, speech-to-text, and Retrieval-Augmented Generation (RAG) techniques to enable natural, human-like academic or informational Q&A experiences.


**Key Features**

🗂️ Multimodal Input — Supports text, image (OCR), and speech input modes.

* 🔍 Contextual Retrieval — Uses LangChain + FAISS for document search and semantic context matching.
* 🧾 OCR Processing — Extracts text from images or scanned notes using Tesseract OCR.
* 🎙️ Speech-to-Text — Converts voice queries into text using SpeechRecognition API.
* 💬 Conversational Memory — Retains session context for smooth multi-turn interactions.
* ⚙️ Streamlit UI — Simple, user-friendly web interface for uploading documents and chatting with the assistant.


**💡 Usage**

* Upload your document (PDF, image, or text).
* Optionally record or upload voice input for speech-based queries.
* Ask questions — the agent retrieves relevant content, interprets it, and generates concise, contextual answers.

**🧮 Example Use Cases**

* Academic Q&A based on lecture notes or study material
* Conversational document retrieval
* AI-based note summarization
* Voice-assisted learning assistant
